{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    \n",
    "    # load data as a normalized way\n",
    "    \n",
    "    C=10  #number of classes\n",
    "    X = LoadBatch(directory)[b'data'].transpose()\n",
    "    # normalize the raw input data\n",
    "    X_mean = X.mean(axis=1).reshape(-1,1)\n",
    "    X_std = X.std(axis=1).reshape(-1,1)\n",
    "    X = (X-X_mean)/X_std\n",
    "    y = np.array(LoadBatch(directory)[b'labels'])\n",
    "    #one-hot representation of the label for each image\n",
    "    Y = np.eye(C, dtype=int)[y].transpose()\n",
    "    return X, Y, y\n",
    "\n",
    "def LoadBatch(filename):\n",
    "    \"\"\" Copied from the dataset website \"\"\"\n",
    "    with open(filename, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr, ytr = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/data_batch_1\")\n",
    "X2, Y2, y2 = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/data_batch_2\")\n",
    "X3, Y3, y3 = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/data_batch_3\")\n",
    "X4, Y4, y4 = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/data_batch_4\")\n",
    "X5, Y5, y5 = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/data_batch_5\")\n",
    "Xtr=np.hstack((Xtr,X2,X3,X4,X5))\n",
    "Ytr=np.hstack((Ytr,Y2,Y3,Y4,Y5))\n",
    "ytr=np.hstack((ytr,y2,y3,y4,y5))\n",
    "Xva=Xtr[0:,0:5000]\n",
    "Yva=Ytr[0:,0:5000]\n",
    "yva=ytr[0:5000]\n",
    "Xtr=Xtr[0:,5000:]\n",
    "Ytr=Ytr[0:,5000:]\n",
    "ytr=ytr[5000:]\n",
    "Xte, Yte, yte = load_data(\"/Users/imogen/Desktop/DD2424/Assignment1/cifar-10-batches-py/test_batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: define some functions in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow( X, Y, P, W, b, h,lamda):\n",
    "        \n",
    "        # centered difference formula \n",
    "        \n",
    "        k = W.shape[0]\n",
    "        d = X.shape[0]\n",
    "\n",
    "        grad_W = np.zeros(W.shape)\n",
    "        grad_b = np.zeros((k, 1))   \n",
    "\n",
    "        for i in range(len(b)):\n",
    "            b_try = np.array(b)\n",
    "            b_try[i] -= h\n",
    "            c1 = compute_cost(X, Y, W, b_try,lamda)\n",
    "\n",
    "            b_try = np.array(b)\n",
    "            b_try[i] += h\n",
    "            c2 = compute_cost(X, Y, W, b_try,lamda)\n",
    "\n",
    "            grad_b[i] = (c2-c1) / (2*h)\n",
    "\n",
    "        for i in range(W.shape[0]):\n",
    "            for j in range(W.shape[1]):\n",
    "                W_try = np.array(W)\n",
    "                W_try[i,j] -= h\n",
    "                c1 = compute_cost(X, Y, W_try, b,lamda)\n",
    "\n",
    "                W_try = np.array(W)\n",
    "                W_try[i,j] += h\n",
    "                c2 = compute_cost(X, Y, W_try, b, lamda)\n",
    "\n",
    "                grad_W[i,j] = (c2-c1) / (2*h)\n",
    "\n",
    "        return [grad_W, grad_b]\n",
    "    \n",
    "def load_label_names(self):\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "        \n",
    "\n",
    "def ini_parameters(X, Y):\n",
    "    m=50\n",
    "    #generate w and b according to the shape of inputs   \n",
    "    std=np.sqrt(1/X.shape[0]) \n",
    "    #std=0.01\n",
    "    np.random.seed(100)\n",
    "    W1 = std*np.random.randn(m,X.shape[0])+0\n",
    "    W2 = std*np.random.randn(Y.shape[0],m)+0\n",
    "    np.random.seed(100)\n",
    "    b1 = std*np.random.randn(m,1)+0\n",
    "    b2 = std*np.random.randn(Y.shape[0],1)+0\n",
    "    return W1,W2,b1,b2\n",
    "    \n",
    "def first_layer(X, W1, b1):\n",
    "    assert(X.shape[0]== W1.shape[1])\n",
    "    s1=np.dot(W1, X)+b1\n",
    "    return s1\n",
    "\n",
    "def activation(s1):\n",
    "    #BN?\n",
    "    h=np.maximum(0,s1)\n",
    "    return h\n",
    "\n",
    "def evaluate_classifier(h, W2, b2):\n",
    "        \n",
    "    # Write a function that evaluates the network function                                            \n",
    "    \n",
    "        assert(h.shape[0]== W2.shape[1])#m nodes\n",
    "        s2=np.dot(W2, h)+b2\n",
    "        return  softmax(s2)\n",
    "    \n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)    \n",
    "\n",
    "\n",
    "def compute_cost(X, Y, y, W1, W2, b1, b2,lamda,svm=False):\n",
    "        \n",
    "    # calculate full loss function\n",
    "    if svm ==False:\n",
    "        L = cross_entropy(X, Y,W1, W2,b1,b2)+(np.sum(W1**2)+np.sum(W2**2))*lamda\n",
    "    elif svm==True:\n",
    "        L = svm_loss(X, y, W, b)+np.sum(W**2)*lamda\n",
    "    return L\n",
    "        \n",
    "    \n",
    "def cross_entropy(X, Y, W1, W2,b1,b2):\n",
    "        \n",
    "    #define cross entropy loss\n",
    "    s1=first_layer(X,W1,b1)\n",
    "    h=activation(s1)\n",
    "    \n",
    "    cross_entropy_loss = np.dot(Y.transpose(), evaluate_classifier(h,W2,b2))               \n",
    "    cross_entropy_loss = np.diag(cross_entropy_loss)\n",
    "    cross_entropy_loss = -np.log(cross_entropy_loss)\n",
    "    loss = np.sum(cross_entropy_loss)/ Y.shape[1]\n",
    "    return loss\n",
    "\n",
    "def svm_loss(X, y, W, b):\n",
    "    \n",
    "    #define svm loss\n",
    "    \n",
    "    assert(X.shape[0]== W.shape[1])\n",
    "    s=np.dot(W, X)+b\n",
    "    l=np.zeros(s.shape)\n",
    "    for i in range(s.shape[0]):\n",
    "        for j in range(s.shape[1]):\n",
    "            l[i][j]= max(0, s[i][j]-s[y[j]][j]+1)\n",
    "    loss=np.sum(l)/ X.shape[1]\n",
    "    return loss   \n",
    "   \n",
    "    \n",
    "def compute_accuracy(X, y, W1, W2, b1, b2):\n",
    "        \n",
    "  #compute the accuracy of the network prediction\n",
    "    X=activation(first_layer(X,W1,b1))\n",
    "    k = np.argmax(evaluate_classifier(X,W2,b2), axis=0)\n",
    "    acc=0\n",
    "    for i in range(X.shape[1]):\n",
    "        if k[i]==y[i]:\n",
    "            acc+=1\n",
    "    acc=acc/X.shape[1]\n",
    "    return acc\n",
    "        \n",
    "    \n",
    "def compute_gradient(X, Y, W1,W2, b1,b2,lamda,svm=False):\n",
    "         \n",
    "    #evaluate the gradients of the cost function for a mini-batch\n",
    "    if svm==False:\n",
    "        H = activation(first_layer(X,W1,b1))\n",
    "        P=evaluate_classifier(H, W2, b2)\n",
    "        g=P-Y\n",
    "        assert(X.shape[1]== g.shape[1])\n",
    "        grad_b2 = np.dot(g,np.ones((X.shape[1],1)))/X.shape[1]\n",
    "        grad_w2 = np.dot(g, H.T)/X.shape[1]+2*lamda*W2\n",
    "        assert(W2.shape[0]== g.shape[0])\n",
    "        g=np.dot(W2.T,g)\n",
    "        assert(g.shape== H.shape)\n",
    "        g=g*np.where(H>0,1,0)\n",
    "        grad_b1=np.dot(g,np.ones((X.shape[1],1)))/X.shape[1]\n",
    "        grad_w1 = np.dot(g, X.T)/X.shape[1]+2*lamda*W1\n",
    "        return grad_w1,grad_w2, grad_b1, grad_b2\n",
    "    \n",
    "    else:\n",
    "        n = X.shape[1]\n",
    "        d = X.shape[0]\n",
    "        K = Y.shape[0]\n",
    "        gradW = np.zeros((K, d))\n",
    "        gradb = np.zeros((K, 1))\n",
    "\n",
    "        for i in range(n):\n",
    "            x = X[:, i]\n",
    "            y_int = np.where(Y[:, [i]].T[0] == 1)[0][0]\n",
    "            s = np.dot(W, X[:, [i]]) + b\n",
    "            for j in range(K):\n",
    "                if j != y_int:\n",
    "                    if max(0, s[j] - s[y_int] + 1) != 0:\n",
    "                        gradW[j] += x\n",
    "                        gradW[y_int] += -x\n",
    "                        gradb[j, 0] += 1\n",
    "                        gradb[y_int, 0] += -1\n",
    "\n",
    "        gradW /= n\n",
    "        gradW += lamda * W\n",
    "        gradb /= n\n",
    "        return gradW, gradb\n",
    "\n",
    "\n",
    "def ComputeGradsNum(X, Y, y, W1, W2, b1, b2, h, lamda):\n",
    "        \n",
    "    # finite difference method\n",
    "    \n",
    "        m = W1.shape[0]\n",
    "        d = X.shape[0]\n",
    "        k = Y.shape[0]\n",
    "\n",
    "        grad_W1 = np.zeros(W1.shape)\n",
    "        grad_W2 = np.zeros(W2.shape)\n",
    "        grad_b1 = np.zeros(b1.shape)\n",
    "        grad_b2 = np.zeros(b2.shape)\n",
    "\n",
    "        c = compute_cost(X, Y, y, W1, W2, b1, b2,lamda,svm=False);\n",
    "        \n",
    "        \n",
    "        for i in range(b1.shape[0]):\n",
    "            \n",
    "            b_try1 = np.array(b1)\n",
    "            b_try2 = np.array(b1)\n",
    "            b_try1[i][0] += h\n",
    "            b_try2[i][0] -= h\n",
    "        \n",
    "            \n",
    "            c1 = compute_cost(X, Y,y, W1,W2,b_try1,b2,lamda,svm=False)\n",
    "            c2 = compute_cost(X, Y, y,W1,W2,b_try2,b2,lamda,svm=False)\n",
    "            grad_b1[i] = (c1-c2) / (2*h)\n",
    "            \n",
    "        for i in range(b2.shape[0]):\n",
    "            \n",
    "            b_try1 = np.array(b2)\n",
    "            b_try2 = np.array(b2)\n",
    "            b_try1[i][0] += h\n",
    "            b_try2[i][0] -= h\n",
    "        \n",
    "            \n",
    "            c1 = compute_cost(X, Y,y, W1,W2,b1,b_try1,lamda,svm=False)\n",
    "            c2 = compute_cost(X, Y, y,W1,W2,b1,b_try2,lamda,svm=False)\n",
    "            grad_b2[i] = (c1-c2) / (2*h)\n",
    "            \n",
    "            \n",
    "        for i in range(W1.shape[0]):\n",
    "            for j in range(W1.shape[1]):\n",
    "                W_try = np.array(W1)\n",
    "                W_try[i,j] += h\n",
    "                c2 = compute_cost(X, Y, y, W_try,W2, b1,b2,lamda,svm=False)\n",
    "                grad_W1[i,j] = (c2-c) / h\n",
    "\n",
    "        for i in range(W2.shape[0]):\n",
    "            for j in range(W2.shape[1]):\n",
    "                W_try = np.array(W2)\n",
    "                W_try[i,j] += h\n",
    "                c2 = compute_cost(X, Y, y, W1, W_try, b1,b2,lamda,svm=False)\n",
    "                grad_W2[i,j] = (c2-c) / h\n",
    "\n",
    "        return [grad_W1,grad_W2, grad_b1,grad_b2]\n",
    "    \n",
    "def montage(W):\n",
    "\t\"\"\" Display the image for each label in W \"\"\"\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tfig, ax = plt.subplots(2,5)\n",
    "\tfor i in range(2):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tim  = W[5*i+j,:].reshape(32,32,3, order='F')\n",
    "\t\t\tsim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "\t\t\tsim = sim.transpose(1,0,2)\n",
    "\t\t\tax[i][j].imshow(sim, interpolation='nearest')\n",
    "\t\t\tax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "\t\t\tax[i][j].axis('off')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    #remember to transpose inputs when using this\n",
    "    #assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        return inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:check gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xmini=Xtr[1:20,1:10]\n",
    "# Ymini=Ytr[:,1:10]\n",
    "\n",
    "# W1,W2, b1,b2= ini_parameters(xmini, Ymini)\n",
    "\n",
    "# gnw1,gnw2, gnb1, gnb2=ComputeGradsNum(X=xmini, Y= Ymini,y=ymini,  W1=W1, W2=W2, b1=b1,b2=b2, h=1e-5,lamda=0)\n",
    "# gaw1,gaw2, gab1,gab2=compute_gradient(X=xmini, Y= Ymini,  W1=W1,W2=W2,b1=b1,b2=b2,lamda=0,svm=False)\n",
    "# print(gnb2-gab2)\n",
    "# print(gnb2-gab2)\n",
    "# print(gnw1-gaw1)\n",
    "# print(gnw2-gaw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4:build a one hidden layer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_hidden_layer_classifier():\n",
    "    \n",
    "    def __init__(self, lamda=0, n_epochs=40, n_batch=100, etamin=1e-5,etamax=0.1,k=8, decay_factor=1,svm=False):\n",
    "    \n",
    "        self.W1, self.W2, self.b1,self.b2= ini_parameters(Xtr, Ytr)\n",
    "        self.P = 0\n",
    "        self.lamda=lamda\n",
    "        self.n_batch=n_batch\n",
    "        self.n_epochs=n_epochs\n",
    "        self.etamin =etamin\n",
    "        self.etamax=etamax\n",
    "        self.eta = self.etamin\n",
    "        self.decay_factor=decay_factor\n",
    "        self.svm=svm\n",
    "        self.k=k\n",
    "    \n",
    "    def fit(self,Xtr, Ytr, ytr, Xva, Yva, yva):\n",
    "        \n",
    "        trainingloss=[]\n",
    "        validationloss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        lr=[]\n",
    "        updatestep=0\n",
    "        for i in range(self.n_epochs):\n",
    "#             best_model=0\n",
    "#             best_acc=0\n",
    "#             l1=compute_cost(Xtr, Ytr, ytr, self.W1,self.W2, self.b1, self.b2, self.lamda,self.svm)\n",
    "#             trainingloss.append(l1)\n",
    "#             acc=compute_accuracy(Xtr, ytr, self.W1,self.W2, self.b1, self.b2)\n",
    "#             trainingaccuracy.append(acc)\n",
    "#             print(\"the\",format(i),\"epoch for training data,\",\" the loss is\", l1, \"the accuracy is \",acc)\n",
    "#             l2=compute_cost(Xva, Yva, yva, self.W1,self.W2, self.b1, self.b2, self.lamda,self.svm)\n",
    "#             validationloss.append(l2)\n",
    "#             acc=compute_accuracy(Xva, yva, self.W1,self.W2, self.b1, self.b2)\n",
    "# #             if acc>best_acc:\n",
    "# #                 best_model=i\n",
    "# #                 best_acc=acc\n",
    "# #                 self.best_w=self.W\n",
    "# #                 self.best_b=self.b\n",
    "#             validationaccuracy.append(acc)\n",
    "#             print(\"the\",format(i),\"epoch for validation data,\",\" the loss is\", l2, \"the accuracy is \",acc)\n",
    "            \n",
    "            Xtr, ytr=iterate_minibatches(Xtr.T, ytr, Xtr.shape[1], shuffle=True)\n",
    "            Xtr=Xtr.T\n",
    "            Ytr = np.eye(10, dtype=int)[ytr].transpose()\n",
    "            \n",
    "\n",
    "            batch_size=Xtr.shape[1]//self.n_batch\n",
    "            for j in range(batch_size):\n",
    "                \n",
    "                x_mini_batch = Xtr.T[j*self.n_batch:(j+1)*self.n_batch]\n",
    "                Y_mini_batch = Ytr.T[j*self.n_batch:(j+1)*self.n_batch]\n",
    "                gradw1,gradw2, gradb1,gradb2 = compute_gradient(x_mini_batch.T, Y_mini_batch.T, self.W1,self.W2, self.b1, self.b2, self.lamda,self.svm)\n",
    "                self.W1-= gradw1*self.eta\n",
    "                self.b1-= gradb1*self.eta\n",
    "                self.W2-= gradw2*self.eta\n",
    "                self.b2-= gradb2*self.eta\n",
    "                ns=self.k*batch_size\n",
    "                if i%(2*self.k)<self.k:\n",
    "                     self.eta =self.etamin + (j+(i%self.k)*batch_size)*(self.etamax-self.etamin)/ns\n",
    "                else:\n",
    "                    self.eta =self.etamax - (j+(i%self.k)*batch_size)*(self.etamax-self.etamin)/ns\n",
    "                lr.append(self.eta)\n",
    "                updatestep+=1\n",
    "#         x = range(1,self.n_epochs+1)    \n",
    "#         plt.plot(x, trainingloss,  label = \"training loss\" )\n",
    "#         plt.plot(x, validationloss, label = \"validation loss\" )\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"loss\")\n",
    "#         plt.legend()\n",
    "#         plt.title('training and validation loss') \n",
    "#         plt.show()\n",
    "        \n",
    "#         x = range(1,self.n_epochs+1)    \n",
    "#         plt.plot(x, trainingaccuracy,  label = \"training accuracy\" )\n",
    "#         plt.plot(x, validationaccuracy, label = \"validation accuracy\" )\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"accuracy\")\n",
    "#         plt.legend()\n",
    "#         plt.title('training and validation accuracy') \n",
    "#         plt.show()\n",
    "        \n",
    "#         x = range(1,updatestep+1)    \n",
    "#         plt.plot(x, lr, label = \"eta\" )\n",
    "#         plt.xlabel(\"update step\")\n",
    "#         plt.ylabel(\"learning rate\")\n",
    "#         plt.legend()\n",
    "#         plt.title('learning rate') \n",
    "#         plt.show()\n",
    "        \n",
    "        #print(self.W,self.b)\n",
    "        #montage(self.W)  \n",
    "#       print(\"the best model is in\",best_model, \"the accuracy is\",best_acc)\n",
    "        \n",
    "    def predict(self, Xte, Yte, yte):\n",
    "        \n",
    "        l=compute_cost(Xte, Yte,yte, self.W1,self.W2, self.b1, self.b2, self.lamda,self.svm)\n",
    "        acc=compute_accuracy(Xte, yte, self.W1,self.W2, self.b1, self.b2)\n",
    "#         print(\"the loss is\", l, \"the accuracy is \",acc)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5:find good values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lamda(l_max,l_min,i):\n",
    "    np.random.seed(i*100)\n",
    "    l=l_min+(l_max-l_min)* np.random.rand()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when l is -2.9023729921453505 the test accuracy is 0.5118\n",
      "when l is -2.913190116418069 the test accuracy is 0.5104\n",
      "when l is -2.1047354862966596 the test accuracy is 0.5095\n",
      "when l is -3.0977548625998903 the test accuracy is 0.5121\n",
      "when l is -2.6625421823316504 the test accuracy is 0.5103\n",
      "when l is -2.612640945989764 the test accuracy is 0.5114\n",
      "when l is -3.935265449644322 the test accuracy is 0.5107\n",
      "when l is -3.6869072707358255 the test accuracy is 0.5137\n",
      "when l is -3.837242449441949 the test accuracy is 0.5079\n",
      "when l is -3.1335969431465314 the test accuracy is 0.509\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    l1=find_lamda(-2,-4,i)\n",
    "    c=one_hidden_layer_classifier(lamda=10**l1, n_epochs=8, n_batch=100, etamin=1e-5,etamax=0.1,k=2)\n",
    "    c.fit(Xtr, Ytr, ytr, Xva, Yva, yva)\n",
    "    print(\"when l is\",l1,\"the test accuracy is\",c.predict( Xte, Yte, yte))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when ns is 100 the test accuracy is 0.4772\n",
      "when ns is 200 the test accuracy is 0.501\n",
      "when ns is 300 the test accuracy is 0.5054\n",
      "when ns is 400 the test accuracy is 0.5112\n",
      "when ns is 500 the test accuracy is 0.5115\n",
      "when ns is 600 the test accuracy is 0.519\n",
      "when ns is 700 the test accuracy is 0.5179\n",
      "when ns is 800 the test accuracy is 0.518\n",
      "when ns is 900 the test accuracy is 0.519\n",
      "when ns is 1000 the test accuracy is 0.5192\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    k=i+1\n",
    "    c=one_hidden_layer_classifier(lamda=10**(-2.82638), n_epochs=2*k, n_batch=100, etamin=1e-5,etamax=0.1,k=k)\n",
    "    c.fit(Xtr, Ytr, ytr, Xva, Yva, yva)\n",
    "    print(\"when ns is\",100*k,\"the test accuracy is\",c.predict( Xte, Yte, yte))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when ns is 100 the test accuracy is 0.5157\n",
      "when ns is 200 the test accuracy is 0.5183\n",
      "when ns is 300 the test accuracy is 0.5221\n",
      "when ns is 400 the test accuracy is 0.5205\n",
      "when ns is 500 the test accuracy is 0.5205\n",
      "when ns is 600 the test accuracy is 0.5204\n",
      "when ns is 700 the test accuracy is 0.5261\n",
      "when ns is 800 the test accuracy is 0.5272\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    k=i+1\n",
    "    c=one_hidden_layer_classifier(lamda=10**(-2.82638), n_epochs=10*k, n_batch=100, etamin=1e-5,etamax=0.1,k=k)\n",
    "    c.fit(Xtr, Ytr, ytr, Xva, Yva, yva)\n",
    "    print(\"when ns is\",100*k,\"the test accuracy is\",c.predict( Xte, Yte, yte))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c=one_hidden_layer_classifier(lamda=10** (-3.195509725199781), n_epochs=20, n_batch=100, etamin=1e-5,etamax=0.1,k=5)\n",
    "c.fit(Xtr, Ytr, ytr, Xva, Yva, yva)\n",
    "c.predict(Xte, Yte, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
